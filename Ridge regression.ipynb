{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2537dc72-5d26-41bd-b13f-1e2d1145e5b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql\n",
    "import pyspark.streaming\n",
    "import pyspark.mllib\n",
    "import pyspark.ml\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b59a83-476f-480b-a2ac-8fa004d9e5cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e948ce2-72d6-4edb-b956-6c71ff1912a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfb3185b-c829-44fc-b01d-c7d09f958fb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('spark_for_conservative_credit_score') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory','8G') \\\n",
    "    .config('spark.driver.maxResultSize', '2G') \\\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config('spark.kryoserializer.buffer.max', '800M')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a97aec67-9896-4d64-8542-de0ca8a8b8cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f396dd64-db5e-488d-8c86-9f4b485d1541",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = spark.read.csv('/data/paper_train1.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e90bfa0d-defa-4153-8c88-162261fcc466",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# loading train data set\n",
    "file_location = \"/data/paper_train1.csv\"\n",
    "file_type = \"csv\"\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "train = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location).cache()\n",
    "\n",
    "# loading test data set\n",
    "file_location = \"/data/paper_valid1.csv\"\n",
    "file_type = \"csv\"\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "valid = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c08aabc-81be-45df-aeb3-877e7ac4b837",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cddaa9ee-0c9a-43fd-a52e-bd88dab9683f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "train = train.withColumn(\"loan_amnt\", train.loan_amnt.cast(\"float\"))\\\n",
    "             .withColumn(\"emp_length\", train.emp_length.cast(\"float\"))\\\n",
    "             .withColumn(\"annual_inc\", train.annual_inc.cast(\"float\"))\\\n",
    "             .withColumn(\"dti\", train.dti.cast(\"float\"))\\\n",
    "             .withColumn(\"delinq_2yrs\", train.delinq_2yrs.cast(\"float\"))\\\n",
    "             .withColumn(\"revol_util\", regexp_replace(\"revol_util\", \"%\", \"\").cast(\"float\"))\\\n",
    "             .withColumn(\"total_acc\", train.total_acc.cast(\"float\"))\\\n",
    "             .withColumn(\"credit_length_in_years\", train.credit_length_in_years.cast(\"float\"))\\\n",
    "             .withColumn(\"int_rate\", regexp_replace(\"int_rate\", \"%\", \"\").cast(\"float\"))\\\n",
    "             .withColumn(\"remain\", train.remain.cast(\"float\"))\\\n",
    "             .withColumn(\"issue_year\", train.issue_year.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_loan_amnt\", train.phi_loan_amnt.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_emp_length\", train.phi_emp_length.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_annual_inc\", train.phi_annual_inc.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_dti\", train.phi_dti.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_delinq_2yrs\", train.phi_delinq_2yrs.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_revol_util\", regexp_replace(\"phi_revol_util\", \"%\", \"\").cast(\"float\"))\\\n",
    "             .withColumn(\"phi_total_acc\", train.phi_total_acc.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_credit_length_in_years\", train.phi_credit_length_in_years.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_int_rate\", regexp_replace(\"phi_int_rate\", \"%\", \"\").cast(\"float\"))\\\n",
    "             .withColumn(\"CRI\", train.CRI.cast(\"float\"))\\\n",
    "             .withColumn(\"train_flag\", train.train_flag.cast(\"float\"))\n",
    "\n",
    "valid = valid.withColumn(\"loan_amnt\", valid.loan_amnt.cast(\"float\"))\\\n",
    "             .withColumn(\"emp_length\", valid.emp_length.cast(\"float\"))\\\n",
    "             .withColumn(\"annual_inc\", valid.annual_inc.cast(\"float\"))\\\n",
    "             .withColumn(\"dti\", valid.dti.cast(\"float\"))\\\n",
    "             .withColumn(\"delinq_2yrs\", valid.delinq_2yrs.cast(\"float\"))\\\n",
    "             .withColumn(\"revol_util\", regexp_replace(\"revol_util\", \"%\", \"\").cast(\"float\"))\\\n",
    "             .withColumn(\"total_acc\", valid.total_acc.cast(\"float\"))\\\n",
    "             .withColumn(\"credit_length_in_years\", valid.credit_length_in_years.cast(\"float\"))\\\n",
    "             .withColumn(\"int_rate\", regexp_replace(\"int_rate\", \"%\", \"\").cast(\"float\"))\\\n",
    "             .withColumn(\"remain\", valid.remain.cast(\"float\"))\\\n",
    "             .withColumn(\"issue_year\", valid.issue_year.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_loan_amnt\", valid.phi_loan_amnt.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_emp_length\", valid.phi_emp_length.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_annual_inc\", valid.phi_annual_inc.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_dti\", valid.phi_dti.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_delinq_2yrs\", valid.phi_delinq_2yrs.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_revol_util\", regexp_replace(\"phi_revol_util\", \"%\", \"\").cast(\"float\"))\\\n",
    "             .withColumn(\"phi_total_acc\", valid.phi_total_acc.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_credit_length_in_years\", valid.phi_credit_length_in_years.cast(\"float\"))\\\n",
    "             .withColumn(\"phi_int_rate\", regexp_replace(\"phi_int_rate\", \"%\", \"\").cast(\"float\"))\\\n",
    "             .withColumn(\"CRI\", valid.CRI.cast(\"float\"))\\\n",
    "             .withColumn(\"train_flag\", valid.train_flag.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efc59b6f-0d8f-4911-a166-80c69ee7d35a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.registerTempTable(\"train\")\n",
    "train.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "valid.registerTempTable(\"valid\")\n",
    "valid.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "print(\" >>>>>>> \" + str(train.count())+ \" loans opened by TRAIN data_set for model training!\")\n",
    "print(\" >>>>>>> \" + str(valid.count())+ \" loans opened by VALID data_set for model validation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5719546-3564-4597-9336-7789356eb507",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\" == imbalancement of the loan train and valid datasets  ==\")\n",
    "print(\" >>>>>>> Train dataset: \" + str(train.groupby('default_loan').count().collect()))\n",
    "print(\" >>>>>>> Test dataset: \" + str(valid.groupby('default_loan').count().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8221c1d0-53b4-4c67-a1a7-5a5b8d335ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the response and predictor variables and set up regression models with train and test datasets.\n",
    "Y = \"default_loan\"\n",
    "\n",
    "categoricals = [\"phi_term_month\", \"home_ownership\", \"purpose\", \"addr_state\", \"verification_status\", \"application_type\"]\n",
    "numerics = [\"CRI\", \"phi_loan_amnt\", \"phi_emp_length\", \"phi_annual_inc\", \"phi_dti\", \"phi_delinq_2yrs\", \"phi_revol_util\", \"phi_total_acc\", \"phi_credit_length_in_years\", \"phi_int_rate\"]\n",
    "X = categoricals + numerics\n",
    "\n",
    "# now we can save the valid to use it as an imput data for our final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "970a484c-9360-4257-a985-23f6d20bc3f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85e85bdf-b401-4bd6-a604-475e8600b705",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (1) define the model function\n",
    "# to build Grid of GLM models and Standardization + CrossValidation\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from plotnine.data import meat\n",
    "from mizani.breaks import date_breaks\n",
    "from mizani.formatters import date_format\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer, OneHotEncoder, Imputer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# setting the parameters\n",
    "maxIter = 10\n",
    "elasticNetParam = 0\n",
    "regParam = 0.3\n",
    "  \n",
    "  ## we start with mlflow.start_run() which essentially start tracking what we are doing in this notebook in databricks\n",
    "with mlflow.start_run():\n",
    "    labelCol = \"default_loan\"\n",
    "    indexers = list(map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = \"keep\"), categoricals))\n",
    "    ohes = list(map(lambda c: OneHotEncoder(inputCol=c + \"_idx\", outputCol=c+\"_class\"), categoricals))\n",
    "    imputers = Imputer(inputCols = numerics, outputCols = numerics)\n",
    "    featureCols = list(map(lambda c: c+\"_class\", categoricals)) + numerics\n",
    "    model_matrix_stages = indexers + ohes + \\\n",
    "                          [imputers] + \\\n",
    "                          [VectorAssembler(inputCols=featureCols, outputCol=\"features\"), \\\n",
    "                           StringIndexer(inputCol= labelCol, outputCol=\"label\")]\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\",\n",
    "                            outputCol=\"scaledFeatures\",\n",
    "                            withStd=True,\n",
    "                            withMean=True)\n",
    "    \n",
    "    ## here, we build the logistic regression model with parameters equal to variables for elasticNet regression\n",
    "    lr = LogisticRegression(maxIter=maxIter, elasticNetParam=elasticNetParam, regParam=regParam, featuresCol = \"scaledFeatures\")\n",
    "    \n",
    "    ##now, we define a pipline which includes everything from standardazing the data, imputing missing values and encoding for categorical columns\n",
    "    pipeline = Pipeline(stages=model_matrix_stages+[scaler]+[lr])\n",
    "    \n",
    "    glm_model = pipeline.fit(train)\n",
    "    \n",
    "    ## Log Params and Model\n",
    "    ## The important part for mlflow of model tracking and reproduceability of the input parameters that we may want to review and take an action.  \n",
    "    mlflow.log_param(\"algorithm\", \"SparkML_GLM_regression\") # we put a name for the algorithm that we used\n",
    "    mlflow.log_param(\"regParam\", regParam)\n",
    "    mlflow.log_param(\"maxIter\", maxIter)\n",
    "    mlflow.log_param(\"elasticNetParam\", elasticNetParam)\n",
    "    mlflow.spark.log_model(glm_model, \"glm_model\")           # here we log the model itself\n",
    "    \n",
    "    ##Evaluate and Log ROC Curve\n",
    "    lr_summary = glm_model.stages[len(glm_model.stages)-1].summary\n",
    "    roc_pd = lr_summary.roc.toPandas()\n",
    "    fpr = roc_pd[\"FPR\"]\n",
    "    tpr = roc_pd[\"TPR\"]\n",
    "    roc_auc = metrics.auc(roc_pd[\"FPR\"], roc_pd[\"TPR\"])\n",
    "   \n",
    "    ## Set Max F1 Threshold  (for predicting the loan default with a balance between true-positives and false-positives)\n",
    "    fMeasure = lr_summary.fMeasureByThreshold\n",
    "    maxFMeasure = fMeasure.groupBy().max(\"F-Measure\").select(\"max(F-Measure)\").head()\n",
    "    madFMeasure = maxFMeasure[\"max(F-Measure)\"]\n",
    "    fMeasure = fMeasure.toPandas()\n",
    "    bestThreshold = float ( fMeasure[ fMeasure[\"F-Measure\"] == maxFMeasure] [\"threshold\"])\n",
    "    lr.setThreshold(bestThreshold)\n",
    "    \n",
    "    \n",
    "     ## Evaluate and Log Metrics  (here we score the customers)\n",
    "    def extract(row):\n",
    "      return (row.remain,) + tuple(row.probability.toArray().tolist()) + (row.label,) + (row.prediction,)\n",
    "\n",
    "    def score(model,data):\n",
    "      pred = model.transform(data).select(\"remain\", \"probability\", \"label\", \"prediction\")\n",
    "      pred = pred.rdd.map(extract).toDF([\"remain\", \"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "      return pred\n",
    "\n",
    "    def auc(pred):\n",
    "      metric = BinaryClassificationMetrics(pred.select(\"p1\", \"label\").rdd)\n",
    "      return metric.areaUnderROC\n",
    "    \n",
    "   \n",
    "    glm_train = score(glm_model, train)\n",
    "    glm_valid = score(glm_model, valid)\n",
    "    \n",
    "    glm_train.registerTempTable(\"glm_train\")\n",
    "    glm_valid.registerTempTable(\"glm_valid\")\n",
    "    \n",
    "    print( \"GLM Training AUC :\" + str( auc(glm_train)))\n",
    "    print( \"GLM Validation AUC :\" + str(auc(glm_valid)))\n",
    "    \n",
    "    ## here we log the auc values and the area under the curve for the models metrics as we defined before for training as well as validation dataset\n",
    "    mlflow.log_metric(\"train_auc\", auc(glm_train))\n",
    "    mlflow.log_metric(\"valid_auc\", auc(glm_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3a9acb8-2be5-48fb-bb70-c7f0309284fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = glm_valid.toPandas()\n",
    "txt = 'This table represents the \"CONFUSION MATRIX\" from Ridge Regression'\n",
    "print(txt.title())\n",
    "pd.crosstab(pandas_df.label, pandas_df.prediction, values=pandas_df.remain, aggfunc=\"count\").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b8c56be-111b-4d31-a80e-8d146db8ef78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_sum_net = glm_valid.groupBy(\"label\", \"prediction\").agg((sum(col(\"remain\"))).alias(\"sum_net\")).toPandas()\n",
    "txt = 'This table represents the \"SUM NET\" from Ridge Regression'\n",
    "print(txt.title())\n",
    "pd.crosstab(pandas_df_sum_net.label, pandas_df_sum_net.prediction, values=pandas_df_sum_net.sum_net , aggfunc=\"sum\").round(2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Ridge regression",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
